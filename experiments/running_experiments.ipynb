{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f27f2a",
   "metadata": {},
   "source": [
    "# Deep Learning final project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "867c3dce",
   "metadata": {},
   "source": [
    "## **Index**\n",
    "\n",
    "- Introduction\n",
    "- Data preprocesing (Custom Data set and Waveform extraction)\n",
    "    - Sentiments and music sample creation logic\n",
    "    - Waveform and labeling strategy for PyTorch dataset\n",
    "- State of art replication\n",
    "    - ESC50 Classification Zeroshot (fuse and infuse) infuse aumentado\n",
    "    - GTZAN Classification (music, music speech) text augmentation\n",
    "    - ESC 50/GTZAN Text to audio retrieval\n",
    "- Experiments\n",
    "    - Zeroshot created sentiments data set classification\n",
    "    - Finetune GTZAN music classification\n",
    "    - Finetune Sentiments classification \n",
    "- Discusion\n",
    "- Conclusion \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "976f1613",
   "metadata": {},
   "source": [
    "# Data preprocesing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "367962f9",
   "metadata": {},
   "source": [
    "## Custom data set creation Overview\n",
    "\n",
    "dataset_creator creates a music sentiment dataset, segmenting input audio files into 10-second chunks and associating each segment with sentiments (Epic, Happy, Sad, Suspense). It leverages `PyTorch's Dataset ` and the `librosa library `. The script organizes segments into an output folder and generates a CSV file (all_music_labels.csv) containing metadata such as file names and sentiments. The dataset structure is tailored for sentiment analysis tasks. Users can customize input audio paths, sentiments, and output locations. The code uses the pydub library for audio processing and pandas for CSV creation, providing a convenient approach to building labeled music datasets for sentiment analysis.\n",
    "This data set is going to be used for some classification tasks.\n",
    "\n",
    "## Audio Datasets Overview\n",
    "\n",
    "audio_datasets provides utility classes for audio datasets used with the CLAP (Contrastive Learning of Audio Representations) model. The classes leverage PyTorch's `Dataset` and the `librosa` library for audio processing.\n",
    "\n",
    "### Dataset Overview:\n",
    "\n",
    "1. **ESC50Dataset:**\n",
    "   - Contains environmental sound recordings.\n",
    "   - Metadata includes categories like \"dog bark,\" \"rain,\" etc.\n",
    "\n",
    "\n",
    "2. **GTZANDataset:**\n",
    "   - Contains audio tracks from the GTZAN music genre dataset.\n",
    "   - Metadata includes genres such as \"blues,\" \"pop,\" etc.\n",
    "\n",
    "\n",
    "3. **MusicSentimentDataset:**\n",
    "   - Contains audio clips labeled with sentiment (e.g., happy, sad).\n",
    "   - Metadata includes sentiment labels and filenames.\n",
    "   \n",
    "\n",
    "### Usage Example:\n",
    "\n",
    "To use these classes, instantiate an object of the desired dataset class, providing the path to the metadata CSV file and the directory containing audio files. Access audio samples, display waveforms, play audio, and more.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "# Instantiate ESC50Dataset\n",
    "esc50_dataset = ESC50Dataset(metadata_path='path/to/esc50_metadata.csv', audio_dir='path/to/audio/files')\n",
    "\n",
    "# Display waveform of the first audio file\n",
    "esc50_dataset.display_waveform(idx=0)\n",
    "\n",
    "# Play audio in Jupyter notebook\n",
    "esc50_dataset.play_audio_in_jupyter(idx=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d80e944d",
   "metadata": {},
   "source": [
    "# State of the Art Replication "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddaf7286",
   "metadata": {},
   "source": [
    "This section aims to replicate and extend state-of-the-art experiments in audio classification using the Contrastive Learning of Audio Representations (CLAP) model. Leveraging `PyTorch`, `librosa` , and the Hugging Face transformers library, we replicate experiments on two widely used audio datasets: ESC50 and GTZAN. Our approach encompasses zero-shot classification, fusion, and infusion techniques utilizing different CLAP pre-trained models. We explore the effectiveness of contrastive learning in capturing rich audio representations, comparing the performance of distinct transformer architectures. The experiments serve as a foundation for further investigations and custom applications in audio analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdf048e8",
   "metadata": {},
   "source": [
    "## Audio Classifier Overview\n",
    "\n",
    "The `AudioClassifier` class facilitates audio classification experiments using the CLAP (Contrastive Learning of Audio Representations) model. This class is equipped with functionalities for dataset initialization, model loading, prediction, and evaluation.\n",
    "\n",
    "### Example of usage\n",
    "```python\n",
    "audio_classifier = AudioClassifier(\n",
    "    metadata_path='path/to/metadata.csv',\n",
    "    audio_dir='path/to/audio/files',\n",
    "    dataset_class='ESC50',\n",
    "    model_id='laion/clap-htsat-fused',\n",
    "    text_augmentation=True\n",
    ")\n",
    "\n",
    "\n",
    "good_predictions, bad_predictions, accuracy, true_labels, pred_labels = audio_classifier.predict_and_evaluate(batch_size=32)\n",
    "audio_classifier.plot_predictions_by_class(good_predictions, bad_predictions)\n",
    "audio_classifier.get_confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17a2afa3",
   "metadata": {},
   "source": [
    "## ESC 50 Zero Shot classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZERO SHOT ESC 50_ htsat-fused, no text augmentation\n",
    "\n",
    "from audio_classification import AudioClassifier\n",
    "\n",
    "esc50_zs_fus = AudioClassifier(\n",
    "    metadata_path = \"audios/ESC-50/ESC-50-master/meta/esc50.csv\",\n",
    "    audio_dir = \"audios/ESC-50/ESC-50-master/audio\",\n",
    "    dataset_class = 'ESC50',\n",
    "    model_id = \"laion/clap-htsat-fused\",\n",
    "    text_augmentation= False)\n",
    "\n",
    "good_predictions_esc50_zs_fus, bad_predictions_esc50_zs_fus, accuaracy_esc50_zs_fus, true_labels_esc50_zs_fus, pred_labels_esc50_zs_fus = esc50_zs_fus.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "esc50_zs_fus.plot_predictions_by_class(good_predictions_esc50_zs_fus, bad_predictions_esc50_zs_fus)\n",
    "esc50_zs_fus.get_confusion_matrix(true_labels_esc50_zs_fus, pred_labels_esc50_zs_fus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZERO SHOT ESC050 htsat-unfused, no text augmentation\n",
    "\n",
    "from audio_classification import AudioClassifier\n",
    "\n",
    "esc50_zs_unf = AudioClassifier(\n",
    "    metadata_path = \"audios/ESC-50/ESC-50-master/meta/esc50.csv\",\n",
    "    audio_dir = \"audios/ESC-50/ESC-50-master/audio\",\n",
    "    dataset_class = 'ESC50',\n",
    "    model_id = \"laion/clap-htsat-unfused\",\n",
    "    text_augmentation= False)\n",
    "\n",
    "good_predictions_esc50_zs_unf, bad_predictions_esc50_zs_unf, accuaracy_esc50_zs_unf, true_labels_esc50_zs_unf, pred_labels_esc50_zs_unf = esc50_zs_unf.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "esc50_zs_unf.plot_predictions_by_class(good_predictions_esc50_zs_unf, bad_predictions_esc50_zs_unf)\n",
    "esc50_zs_unf.get_confusion_matrix(true_labels_esc50_zs_unf, pred_labels_esc50_zs_unf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZERO SHOT ESC050 htsat-unfused, text augmentation\n",
    "\n",
    "from audio_classification import AudioClassifier\n",
    "\n",
    "esc50_zs_unf_aug = AudioClassifier(\n",
    "    metadata_path = \"audios/ESC-50/ESC-50-master/meta/esc50.csv\",\n",
    "    audio_dir = \"audios/ESC-50/ESC-50-master/audio\",\n",
    "    dataset_class = 'ESC50',\n",
    "    model_id = \"laion/clap-htsat-unfused\",\n",
    "    text_augmentation= True)\n",
    "\n",
    "good_predictions_esc50_zs_unf_aug, bad_predictions_esc50_zs_unf_aug, accuaracy_esc50_zs_unf_aug, true_labels_esc50_zs_unf_aug, pred_labels_esc50_zs_unf_aug = esc50_zs_unf_aug.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "esc50_zs_unf_aug.plot_predictions_by_class(good_predictions_esc50_zs_unf_aug, bad_predictions_esc50_zs_unf_aug)\n",
    "esc50_zs_unf_aug.get_confusion_matrix(true_labels_esc50_zs_unf_aug, pred_labels_esc50_zs_unf_aug)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed02c4c8",
   "metadata": {},
   "source": [
    "## GTZAN Zero Shot genres classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52189bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_zs_lcm = AudioClassifier(\n",
    "    metadata_path = \"audios/GTZAN/features_30_sec.csv\",\n",
    "    audio_dir = \"audios/GTZAN/genres_original\",\n",
    "    dataset_class = 'GTZAN',\n",
    "    model_id = \"laion/larger_clap_music\",\n",
    "    text_augmentation= True)\n",
    "\n",
    "good_predictions_GT_zs_lcm, bad_predictions_GT_zs_lcm, accuaracy_GT_zs_lcm, true_labels_GT_zs_lcm, pred_labels_GT_zs_lcm = GT_zs_lcm.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "GT_zs_lcm.plot_predictions_by_class(good_predictions_GT_zs_lcm, bad_predictions_GT_zs_lcm)\n",
    "GT_zs_lcm.get_confusion_matrix(true_labels_GT_zs_lcm, pred_labels_GT_zs_lcm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_zs_lcmspch = AudioClassifier(\n",
    "    metadata_path = \"audios/GTZAN/features_30_sec.csv\",\n",
    "    audio_dir = \"audios/GTZAN/genres_original\",\n",
    "    dataset_class = 'GTZAN',\n",
    "    model_id = \"laion/larger_clap_music_and_speech\",\n",
    "    text_augmentation= True)\n",
    "\n",
    "good_predictions_GT_zs_lcmspch, bad_predictions_GT_zs_lcmspch, accuaracy_GT_zs_lcmspch, true_labels_GT_zs_lcmspch, pred_labels_GT_zs_lcmspch = GT_zs_lcmspch.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "GT_zs_lcmspch.plot_predictions_by_class(good_predictions_GT_zs_lcmspch, bad_predictions_GT_zs_lcmspch)\n",
    "GT_zs_lcmspch.get_confusion_matrix(true_labels_GT_zs_lcmspch, pred_labels_GT_zs_lcmspch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d58318cc",
   "metadata": {},
   "source": [
    "## Text to audio Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad21058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_datasets.audio_dataset import ESC50Dataset, GTZANDataset, MusicSentimentDataset\n",
    "from text_to_audio import Text2Audio\n",
    "metadata_path = 'audios/ESC-50/ESC-50-master/meta/esc50.csv'\n",
    "audio_dir = 'audios/ESC-50/ESC-50-master/audio/'\n",
    "dataset = ESC50Dataset(metadata_path, audio_dir)\n",
    "dataset_class = 'ESC50'\n",
    "text_query = 'a dog barking' # We can put any query\n",
    "\n",
    "model_id = \"laion/clap-htsat-fused\"\n",
    "# model_id = \"laion/larger_clap_music\"\n",
    "\n",
    "# Initialize the Text2Audio\n",
    "text2audio = Text2Audio(metadata_path, audio_dir, dataset_class, model_id, text_query)\n",
    "\n",
    "# Get audio embeddings\n",
    "audio_embeddings = text2audio.get_audio_embeddings()\n",
    "\n",
    "# Get similarities\n",
    "similarities = text2audio.get_similarities(audio_embeddings)\n",
    "\n",
    "# Get top indices\n",
    "top_indices = text2audio.get_top_indices(similarities, k=5)\n",
    "\n",
    "# Print the top 5 most similar audio files and their similarity scores\n",
    "for idx in top_indices:\n",
    "    print(f\"Cosine Similarity: {similarities[idx]}\")\n",
    "    print(f\"Audio file: {dataset.filenames[idx]}\")\n",
    "    print(f\"Label: {dataset.labels[idx]}\")\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5835342f",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3457c91",
   "metadata": {},
   "source": [
    "## Zero Shot Classification for Sentiments in music \n",
    "\n",
    "We will run a test using the pretrained transformer from CLAP, that gives the best result for clasifying genres in the GTZAN data set, tryin to achieve a sentiment classification, using our created data set `MusicSentiment` Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MusicSentiment_zs = AudioClassifier(\n",
    "metadata_path = \"audios/emotions/emotions_music_labels.csv\",\n",
    "audio_dir = \"audios/emotions\",\n",
    "dataset_class = 'MusicSentiment',\n",
    "model_id = \"laion/larger_clap_music_and_speech\", \n",
    "text_augmentation= True)\n",
    "\n",
    "\n",
    "good_predictions_MusicSentiment_zs, bad_predictions_MusicSentiment_zs, accuaracy_MusicSentiment_zs, true_labels_MusicSentiment_zs, pred_labels_MusicSentiment_zs = MusicSentiment_zs.predict_and_evaluate(\n",
    "    batch_size=32)\n",
    "MusicSentiment_zs.plot_predictions_by_class(good_predictions_MusicSentiment_zs, bad_predictions_MusicSentiment_zs)\n",
    "MusicSentiment_zs.get_confusion_matrix(true_labels_MusicSentiment_zs, pred_labels_MusicSentiment_zs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "284e88e9",
   "metadata": {},
   "source": [
    "## Fine-Tune Audio Classifier Overview\n",
    "\n",
    "The provided code introduces a `FineTuneAudioClassifier` class for fine-tuning a pre-trained CLAP (Contrastive Learning of Audio Representations) model. Key components include:\n",
    "\n",
    "- Loading audio datasets (`ESC50`, `GTZAN`, or `MusicSentiment`) using respective classes.\n",
    "- Initializing the CLAP processor and model.\n",
    "- Handling data splitting for training, testing, and validation.\n",
    "- Freezing and unfreezing model parameters for efficient training.\n",
    "- Utilizing PyTorch, Transformers, and other libraries for implementation.\n",
    "\n",
    "#### Running the Experiments\n",
    "\n",
    "1. Initialize the `FineTuneAudioClassifier` class with dataset details and the pre-trained CLAP model.\n",
    "2. Prepare datasets using the `prepare_datasets` method.\n",
    "3. Define the classifier model, for example, using `nn.Linear`.\n",
    "4. Train the model using the `train` method, specifying hyperparameters like `num_epochs`, `batch_size`, and `lr`.\n",
    "5. Evaluate the trained model using the `evaluate` method.\n",
    "6. Visualize training and validation losses with `plot_loss` and confusion matrices with `plot_confusion_matrix`.\n",
    "7. Further analysis can be performed, such as predicting and evaluating specific batches.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "# Initialize the FineTuneAudioClassifier\n",
    "metadata_path = \"audios/GTZAN/features_30_sec.csv\"\n",
    "audio_dir = \"audios/GTZAN/genres_original\"\n",
    "dataset_class = 'GTZAN'\n",
    "model_id = \"laion/larger_clap_music_and_speech\"\n",
    "fine_tune_audio_classifier = FineTuneAudioClassifier(metadata_path, audio_dir, dataset_class, model_id)\n",
    "\n",
    "# Prepare datasets\n",
    "fine_tune_audio_classifier.prepare_datasets()\n",
    "\n",
    "# Define the classifier model\n",
    "classifier = nn.Linear(512, fine_tune_audio_classifier.num_classes)\n",
    "\n",
    "# Train the model\n",
    "train_losses, valid_losses = fine_tune_audio_classifier.train(classifier, num_epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_loss, accuracy, true_labels, pred_labels = fine_tune_audio_classifier.evaluate(classifier)\n",
    "\n",
    "# Plot loss and confusion matrix\n",
    "fine_tune_audio_classifier.plot_loss(train_losses, valid_losses)\n",
    "fine_tune_audio_classifier.plot_confusion_matrix(true_labels, pred_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2e6fd1",
   "metadata": {},
   "source": [
    "## GTZAN Fine tune Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e841c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune_audio_classification import FineTuneAudioClassifier\n",
    "\n",
    "metadata_path = \"audios/GTZAN/features_30_sec.csv\"\n",
    "audio_dir = \"audios/GTZAN/genres_original\"\n",
    "dataset_class = 'GTZAN'\n",
    "model_id = \"laion/larger_clap_music_and_speech\"\n",
    "GTZAN_fine_tune = FineTuneAudioClassifier(metadata_path, audio_dir, dataset_class, model_id)\n",
    "\n",
    "#Prepare datasets\n",
    "GTZAN_fine_tune.prepare_datasets()\n",
    "\n",
    "#Define the classifier model\n",
    "classifier_MusicSentiment_fine_tune = nn.Linear(512, GTZAN_fine_tune.num_classes)\n",
    "\n",
    "# Train the model\n",
    "train_losses_MusicSentiment_fine_tune, valid_losses_MusicSentiment_fine_tune = GTZAN_fine_tune.train(classifier_MusicSentiment_fine_tune, num_epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_loss_GTZAN_fine_tune, accuracy_GTZAN_fine_tune, true_labels_GTZAN_fine_tune, pred_labels_GTZAN_fine_tune = GTZAN_fine_tune.evaluate(classifier_MusicSentiment_fine_tune)\n",
    "\n",
    "# train_losses, valid_losses, elapsed_time = fine_tune_audio_classifier.train(classifier)\n",
    "GTZAN_fine_tune.plot_loss(train_losses_MusicSentiment_fine_tune, valid_losses_MusicSentiment_fine_tune)\n",
    "GTZAN_fine_tune.plot_confusion_matrix(true_labels_GTZAN_fine_tune, pred_labels_GTZAN_fine_tune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9624175d",
   "metadata": {},
   "source": [
    "## Sentiments in Music Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"audios/emotions/emotions_music_labels.csv\",\n",
    "audio_dir = \"audios/emotions\",\n",
    "dataset_class = 'MusicSentiment',\n",
    "model_id = \"laion/larger_clap_music_and_speech\",\n",
    "MusicSentiment_fine_tune = FineTuneAudioClassifier(metadata_path, audio_dir, dataset_class, model_id)\n",
    "\n",
    "#Prepare datasets\n",
    "MusicSentiment_fine_tune.prepare_datasets()\n",
    "\n",
    "#Define the classifier model\n",
    "classifier_MusicSentiment_fine_tune = nn.Linear(512, MusicSentiment_fine_tune.num_classes)\n",
    "\n",
    "# Train the model\n",
    "train_losses_MusicSentiment_fine_tune, valid_losses_MusicSentiment_fine_tune = MusicSentiment_fine_tune.train(classifier_MusicSentiment_fine_tune, num_epochs=2)\n",
    "\n",
    "# Evaluate the model\n",
    "avg_loss_MusicSentiment_fine_tune, accuracy_MusicSentiment_fine_tune, true_labels_MusicSentiment_fine_tune, pred_labels_MusicSentiment_fine_tune = MusicSentiment_fine_tune.evaluate(classifier)\n",
    "\n",
    "# train_losses, valid_losses, elapsed_time = fine_tune_audio_classifier.train(classifier)\n",
    "MusicSentiment_fine_tune.plot_loss(train_losses_MusicSentiment_fine_tune, valid_losses_MusicSentiment_fine_tune)\n",
    "MusicSentiment_fine_tune.plot_confusion_matrix(true_labels_MusicSentiment_fine_tune, pred_labels_MusicSentiment_fine_tune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5d35b5",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
